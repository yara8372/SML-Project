{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing and handling the data\n",
    "\n",
    "Before applying boosting as a model on the data we're gonne preprocess and seperate the data and it's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for just the required import or other types of setup needed for the notebook\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import gen_csv_from_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# df['Lead'].replace(['Male', 'Female'], [0, 1], inplace=True)\n",
    "y,X = df['Lead'], df.drop(columns=['Lead'])# Can any other columns be dropped?\n",
    "# FIXME: (potentially) this can be uneccessary since we already have two data sets of unlabeled examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using a default model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8509615384615384\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier() # default estimator val\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "test_pred = clf.predict(test_data)\n",
    "temp = y_test.to_list()\n",
    "\n",
    "# Getting a score against another splitted dataset\n",
    "correct = 0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == temp[i]:\n",
    "        correct += 1\n",
    "print(correct / len(pred))\n",
    "gen_csv_from_pred(test_pred, \"boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing multiple classifier methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "CategoricalNB\n",
      "ClassifierChain\n",
      "ComplementNB\n",
      "DecisionTreeClassifier\n",
      "DummyClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "GridSearchCV\n",
      "HalvingGridSearchCV\n",
      "HalvingRandomSearchCV\n",
      "HistGradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultiOutputClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "OneVsRestClassifier\n",
      "Pipeline\n",
      "QuadraticDiscriminantAnalysis\n",
      "RFE\n",
      "RFECV\n",
      "RadiusNeighborsClassifier\n",
      "RandomForestClassifier\n",
      "RandomizedSearchCV\n",
      "SGDClassifier\n",
      "SVC\n",
      "SelfTrainingClassifier\n",
      "StackingClassifier\n",
      "VotingClassifier\n"
     ]
    }
   ],
   "source": [
    "#Generate all estimator with the predict_proba, function required by boosting\n",
    "# yoinked from here: https://stackoverflow.com/questions/30056331/how-to-list-all-scikit-learn-classifiers-that-support-predict-proba\n",
    "# also possible to use CalibratedClassifierCV to make any classifier have such a method\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default AdaBoost: 0.8798076923076923\n",
      "Logistic Regression: 0.8413461538461539\n",
      "Linear Support Vector Machine: 0.8028846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import gen_csv_from_pred\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# df['Lead'].replace(['Male', 'Female'], [0, 1], inplace=True)\n",
    "y,X = df['Lead'], df.drop(columns=['Lead'])\n",
    "logreg = linear_model.LogisticRegression() \n",
    "svc = svm.LinearSVC()\n",
    "models = [None, logreg, svc]\n",
    "model_names = [\"Default AdaBoost\",  \"Logistic Regression\", \"Linear Support Vector Machine\"]\n",
    "param_dicts = [\n",
    "                {\n",
    "                    'n_estimators':[1,5,10,15,25,35,50,100,1000],\n",
    "                    'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "                },\n",
    "                [\n",
    "                    {\n",
    "                    'tol':[1e-5,1e-4,1e-3,1e-2],\n",
    "                    'C':[1e-4,1e-3,1e-2,1e-1,1], \n",
    "                    'solver':('saga','lbfgs'),\n",
    "                    'max_iter':[100,1000,10000]\n",
    "                    },\n",
    "                    {\n",
    "                    'tol':[1e-5,1e-4,1e-3,1e-2],\n",
    "                    'C':[1e-4,1e-3,1e-2,1e-1,1], \n",
    "                    'penalty':('l1','l2'),\n",
    "                    'solver':('liblinear','saga'),\n",
    "                    'max_iter':[100,1000,10000]\n",
    "                    }\n",
    "                ],\n",
    "                [\n",
    "                    {\n",
    "                    'tol':[1e-5,1e-4,1e-3,1e-2],\n",
    "                    'C':[1e-4,1e-3,1e-2,1e-1,1], \n",
    "                    'penalty':['l2'],\n",
    "                    'loss':('hinge','squared_hinge'),\n",
    "                     'max_iter':[100,1000,10000]\n",
    "                    },\n",
    "                    {\n",
    "                    'tol':[1e-5,1e-4,1e-3,1e-2],\n",
    "                    'C':[1e-4,1e-3,1e-2,1e-1,1], \n",
    "                     'max_iter':[100,1000,10000]\n",
    "                    }\n",
    "                ]\n",
    "                ]  \n",
    "gridmodels = []\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "#Ignores stdout from models about convergence\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def test():\n",
    "    for i in range(len(models)):\n",
    "        item = models[i]\n",
    "        param = param_dicts[i]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        preds = []\n",
    "        correct = 0\n",
    "        temp = y_test.to_list()\n",
    "        if item is None:\n",
    "            clf = AdaBoostClassifier() # default estimator val)\n",
    "            testgrid = GridSearchCV(estimator = clf, param_grid = param)\n",
    "            testgrid.fit(X_train, y_train)\n",
    "            gridmodels.append(testgrid)\n",
    "            clf.fit(X_train, y_train)\n",
    "            preds = (clf.predict(X_test))\n",
    "        else:\n",
    "            clf = AdaBoostClassifier(item, algorithm='SAMME')\n",
    "            testgrid = GridSearchCV(estimator = item, param_grid = param,refit=True)\n",
    "            testgrid.fit(X_train, y_train)\n",
    "            gridmodels.append(testgrid)\n",
    "            clf.fit(X_train,y_train)\n",
    "            preds = (clf.predict(X_test)) \n",
    "        \n",
    "        for j in range(len(preds)):\n",
    "            if preds[j] == temp[j]:\n",
    "                correct += 1\n",
    "        print(f\"{model_names[i]}: {correct / len(preds)}\")\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using the results from GridSearchCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator was:  AdaBoostClassifier(learning_rate=0.9)\n",
      "Best score was:  0.8688622754491018\n",
      "Best params was:  {'learning_rate': 0.9, 'n_estimators': 50}\n",
      "Best estimator was:  LogisticRegression(C=0.1, max_iter=1000, tol=1e-05)\n",
      "Best score was:  0.876055118678306\n",
      "Best params was:  {'C': 0.1, 'max_iter': 1000, 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "Best estimator was:  LinearSVC(C=0.001, max_iter=10000, tol=1e-05)\n",
      "Best score was:  0.812279056345141\n",
      "Best params was:  {'C': 0.001, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "for item in gridmodels:\n",
    "    est = item.best_estimator_\n",
    "    scr = item.best_score_\n",
    "    par = item.best_params_\n",
    "    print(\"Best estimator was: \", est)\n",
    "    print(\"Best score was: \",scr)\n",
    "    print(\"Best params was: \",par)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned AdaBoost: 0.8076923076923077\n",
      "Tuned Logistic Regression: 0.7692307692307693\n",
      "Tuned Linear Support Vector Machine: 0.7836538461538461\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import gen_csv_from_pred\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# df['Lead'].replace(['Male', 'Female'], [0, 1], inplace=True)\n",
    "# print(df.info)\n",
    "y,X = df['Lead'], df.drop(columns=['Lead'])\n",
    "logreg = linear_model.LogisticRegression(C=1, penalty='l1', solver='liblinear', tol=0.01,max_iter=10000)\n",
    "svc = svm.LinearSVC(C=0.0001, max_iter=100000, tol=1e-05)\n",
    "models = [None, logreg, svc]\n",
    "model_names = [\"Tuned AdaBoost\",  \"Tuned Logistic Regression\", \"Tuned Linear Support Vector Machine\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    item = models[i]\n",
    "    param = param_dicts[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    preds = []\n",
    "    correct = 0\n",
    "    temp = y_test.to_list()\n",
    "    if item is None:\n",
    "        clf = AdaBoostClassifier(learning_rate=0.9,n_estimators=25) # default estimator val)\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = (clf.predict(X_test))\n",
    "    else:\n",
    "        clf = AdaBoostClassifier(item,algorithm='SAMME')\n",
    "        clf.fit(X_train,y_train)\n",
    "        preds = (clf.predict(X_test)) \n",
    "    \n",
    "    for j in range(len(preds)):\n",
    "        if preds[j] == temp[j]:\n",
    "            correct += 1\n",
    "    print(f\"{model_names[i]}: {correct / len(preds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
