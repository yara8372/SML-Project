{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing and handling the data\n",
    "\n",
    "Before applying linear regression as a model on the data we're gonne preprocess and seperate the data and its' labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "# split train data\n",
    "y = data['Lead']\n",
    "X = data.loc[:, data.columns!='Lead'] # Can any other columns be dropped?\n",
    "# FIXME: (potentially) this can be uneccessary since we already have two data sets of unlabeled examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline for gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_sc = StandardScaler()\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "pipeline = Pipeline(steps=[('skl_sc', skl_sc),\n",
    "                            ('pca', pca),\n",
    "                            ('model', model)])\n",
    "\n",
    "n_components = list(range(1,X_train.shape[1]+1,1))\n",
    "\n",
    "# Hyperparamter C and penalty:\n",
    "C = np.logspace(-4,4,50)\n",
    "penalty = ['l2']\n",
    "\n",
    "params = dict(pca__n_components=n_components, model__C=C, model__penalty=penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gridsearch and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch = GridSearchCV(pipeline, params, refit=True, n_jobs=6)\n",
    "ans = gridSearch.fit(X_train, y_train)\n",
    "y_pred_grid = gridSearch.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9086538461538461\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import decomposition\n",
    "\n",
    "skl_sc = StandardScaler()\n",
    "pca = decomposition.PCA(n_components=gridSearch.best_estimator_.get_params()['pca__n_components'])\n",
    "model = LogisticRegression(C=gridSearch.best_estimator_.get_params()['model__C'],penalty=gridSearch.best_estimator_.get_params()['model__penalty'])\n",
    "pipeline = Pipeline(steps=[('skl_sc', skl_sc),\n",
    "                            ('pca', pca),\n",
    "                            ('model', model)])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "y_test = y_test.to_list()\n",
    "assert len(y_test) == len(y_pred)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Print the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 35.564803062231285\n",
      "Best Number Of Components: 12\n",
      "\n",
      "LogisticRegression(C=35.564803062231285)\n"
     ]
    }
   ],
   "source": [
    "print('Best Penalty:', gridSearch.best_estimator_.get_params()['model__penalty'])\n",
    "print('Best C:', gridSearch.best_estimator_.get_params()['model__C'])\n",
    "print('Best Number Of Components:', gridSearch.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); print(gridSearch.best_estimator_.get_params()['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_csv_from_pred\n",
    "y_pred_file = pipeline.predict(pd.read_csv('data/test.csv'))\n",
    "gen_csv_from_pred(y_pred_file, \"Logistic_regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
